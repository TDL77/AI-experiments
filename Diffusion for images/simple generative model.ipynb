{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bdab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем все, что нужно\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D,AveragePooling2D,Conv2DTranspose, Input, Concatenate, Add, BatchNormalization, Activation, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow_addons as tfa\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "#подключена ли видеокарта?\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa777e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузим датасет\n",
    "\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'C:/users/user/8k',\n",
    "    label_mode= None,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(image_size, image_size),\n",
    "    shuffle=True,\n",
    "    interpolation = 'area',\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "#препроцессинг\n",
    "@tf.function\n",
    "def process(x):\n",
    "    #приводим значения пикселей к [0...1]\n",
    "    return (x/256.0)\n",
    "\n",
    "dataset = dataset.map(process)\n",
    "dataset = dataset.shuffle(500).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38880b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#посмотрим на картинки\n",
    "def imshow(): \n",
    "    n = 5\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    #берем один батч из датасета. проходимся по первым n\n",
    "    for images in dataset.take(1):\n",
    "        for i in range(n):\n",
    "            img = images[i]\n",
    "            ax = plt.subplot(3, n, i + 1 + n)\n",
    "            \n",
    "            plt.imshow(img, cmap='gist_gray')\n",
    "         #   plt.axis('off')\n",
    "            ax.get_yaxis().set_visible(False)           \n",
    "    plt.show()\n",
    "imshow()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdcffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#определим некоторые параметры\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201428cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#как представить данные о времени для сверточного слоя?\n",
    "#лучший способ - sinusoidal embedding\n",
    "\n",
    "embedding_dims = 32\n",
    "embedding_max_frequency = 1000.0\n",
    "\n",
    "def sinusoidal_embedding(x):\n",
    "    embedding_min_frequency = 1.0\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(embedding_max_frequency),\n",
    "            embedding_dims // 2,\n",
    "        )\n",
    "    )\n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908dfeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#немного визуализации\n",
    "example_time = tf.constant([[[[0.3]]]])\n",
    "\n",
    "print(sinusoidal_embedding(example_time).shape)\n",
    "\n",
    "#найдем крайние углы\n",
    "start_angle = tf.acos(max_signal_rate)\n",
    "end_angle = tf.acos(min_signal_rate)\n",
    "\n",
    "example_batch_of_times = tf.range(start_angle, end_angle, delta=0.01)\n",
    "\n",
    "#добавим несколько измерений, чтобы функция заработала\n",
    "for _ in range(3):\n",
    "    example_batch_of_times = tf.expand_dims(example_batch_of_times, axis = -1)\n",
    "\n",
    "\n",
    "example_embeddings = sinusoidal_embedding(example_batch_of_times)\n",
    "\n",
    "print(example_embeddings.shape)\n",
    "\n",
    "#обратно для отображения\n",
    "example_embeddings = tf.squeeze(example_embeddings, axis = [1,2])\n",
    "\n",
    "example_embeddings = example_embeddings.numpy()\n",
    "\n",
    "#отрисовываем\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(example_embeddings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddf6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#код сборки нейросети\n",
    "\n",
    "widths = [32, 64, 96, 128, 256]\n",
    "block_depth = 2\n",
    "\n",
    "#основной \"вычислительный\" блок, на них приходится большинство параметров\n",
    "\n",
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\", activation=keras.activations.swish)(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "#есть два варианта блока на выбор. Работают вроде одинаково\n",
    "\n",
    "'''\n",
    "def ResidualBlock(width):\n",
    "    def apply(x): \n",
    "        \n",
    "        conv1 = Conv2D(width, 3, activation = 'relu', padding = 'same')(x)\n",
    "        convint = Conv2D(width, 3, padding = 'same')(conv1)\n",
    "        convint = BatchNormalization()(convint)\n",
    "        convint = Activation('relu')(convint)\n",
    "        convint = Conv2D(width, 3, padding = 'same')(convint)\n",
    "        convint = BatchNormalization()(convint)\n",
    "        summ = Add()([convint, conv1])\n",
    "        x = Activation('relu')(summ)\n",
    "        return x\n",
    "    return apply\n",
    "'''\n",
    "\n",
    "#понижающий блок\n",
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips, emb = x\n",
    "        height = x.shape[1]\n",
    "        e = layers.UpSampling2D(size=height, interpolation=\"nearest\")(emb)\n",
    "        x = Concatenate()([x, e])\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "#повышающий блок\n",
    "def UpBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_network(image_size, widths, block_depth):\n",
    "    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n",
    "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
    "    \n",
    "    x = noisy_images\n",
    "    emb = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
    "\n",
    "    skips = []\n",
    "    \n",
    "    #сборка нейросети U-net автоматическая. Имеет смысл менять только кол-во блоков и\n",
    "    #архитектуру основного блока. Остальное код сделает за нас)\n",
    "    \n",
    "    for width in widths[:-1]:\n",
    "        x = DownBlock(width, block_depth)([x, skips, emb])\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1])(x)\n",
    "\n",
    "\n",
    "    for width in reversed(widths[:-1]):\n",
    "        x = UpBlock(width, block_depth)([x, skips])\n",
    "        \n",
    "    #для лучшей четкости картинки \n",
    "    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(emb)\n",
    "    x = layers.Concatenate()([x, noisy_images, e])\n",
    "    x = layers.Conv2D(32, kernel_size=1, padding = 'same', activation=keras.activations.swish )(x)\n",
    "\n",
    "    x = layers.Conv2D(3, kernel_size=1,  kernel_initializer=\"zeros\")(x)\n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(self, image_size, widths, block_depth):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization()\n",
    "        self.network = get_network(image_size, widths, block_depth)\n",
    "        \n",
    "        self.optimizer = optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "\n",
    "    def denormalize(self, images):\n",
    "        #обратная задача нормализации\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        \n",
    "        #найдем такой угол, для которого cos будет min и max т.е. 0 и 1\n",
    "        start_angle = tf.acos(max_signal_rate)\n",
    "        end_angle = tf.acos(min_signal_rate)\n",
    "\n",
    "        #переменная diffusion_times сторого от 0 до 1 линейно сдвигает угол от start к end\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        #считаем параметры, основываясь на угол (а по сути на diffusion_times)\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "        # косинусы с синусами не просто так: sin^2(x) + cos^2(x) = 1\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates):\n",
    "        \n",
    "        #предсказавыем весь шум, предсказанный нейросетью\n",
    "        pred_noises = self.network([noisy_images, noise_rates**2])\n",
    "        \n",
    "        #и находим чистые данные, удаляя шум так, будто шаг всего один\n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        \n",
    "        num_images = initial_noise.shape[0]\n",
    "        #найдем размер шага\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # на первом шаге \"noisy image\" чистый шум\n",
    "        # но его signal rate должен быть ненулевым (min_signal_rate)\n",
    "        \n",
    "        next_noisy_images = initial_noise\n",
    "        \n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_images = next_noisy_images\n",
    "            \n",
    "            #время диффузии идет в обратную сторону (1 - step)\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            \n",
    "            #находим параметры\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            \n",
    "            #предсказываем шум и сигнал без шума, 'разделяем' их\n",
    "            pred_noises, pred_images = self.denoise(noisy_images, noise_rates, signal_rates)\n",
    "           \n",
    "            #если отнять stepsize, получим время для следующего шага\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            \n",
    "            #считаем параметры (теперь значение шума меньше, значение сигнала больше)\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(next_diffusion_times)\n",
    "            \n",
    "            #и с новыми параметрами опять собираем зашумленный сигнал\n",
    "            #из того-же шума и того-же сигнала\n",
    "            next_noisy_images = (next_signal_rates * pred_images + next_noise_rates * pred_noises)\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps):\n",
    "        \n",
    "        initial_noise = tf.random.normal(shape=(num_images, image_size, image_size, 3))\n",
    "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, images):\n",
    "        \n",
    "        images = self.normalizer(images, training=True)\n",
    "        \n",
    "        noises = tf.random.normal(shape=(batch_size, image_size, image_size, 3))\n",
    "\n",
    "        \n",
    "        diffusion_times = tf.random.uniform(shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0)\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        \n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            pred_noises, pred_images = self.denoise(noisy_images, noise_rates, signal_rates)\n",
    "            \n",
    "            noise_loss = tf.reduce_mean(tf.abs(noises - pred_noises), axis = [1,2,3])\n",
    "\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "       \n",
    "        return noise_loss\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n",
    "        \n",
    "        generated_images = self.generate(\n",
    "            num_images=num_rows * num_cols,\n",
    "            diffusion_steps=plot_diffusion_steps,\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                index = row * num_cols + col\n",
    "                plt.subplot(num_rows, num_cols, index + 1)\n",
    "                plt.imshow(generated_images[index])\n",
    "                plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "model = DiffusionModel(image_size, widths, block_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализация нормализатора, всегда выполнять перед обучением\n",
    "for a in dataset.take(10):\n",
    "    model.normalizer.adapt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8142b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diffusion_steps = 100\n",
    "model.plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81f351",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#обучаем\n",
    "epochs = 50\n",
    "hist = np.array(np.empty([0]))\n",
    "from IPython.display import clear_output\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "\n",
    "   \n",
    "    midloss = 0\n",
    "    for step, x in enumerate(dataset):\n",
    "        if tf.shape(x)[0] == 64: #проверяем целостность батча\n",
    "            midloss += tf.reduce_mean(model.train_step(x), axis = 0)\n",
    "\n",
    "        if(step == 5):  \n",
    "            clear_output(wait=True)\n",
    "            print('эпоха ' + str(epoch))\n",
    "            print('ошибка: ' + str(float(midloss/5)))\n",
    "           \n",
    "            hist = np.append(hist, float(midloss/5))\n",
    "            plt.plot(np.arange(0,len(hist)), hist)\n",
    "            plt.show()\n",
    "            #model.plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e3fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
